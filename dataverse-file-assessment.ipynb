{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9450507",
   "metadata": {},
   "source": [
    "# Assessment and reporting scripts for the Texas Data Repository\n",
    "\n",
    "## Metadata\n",
    "* *Version*: 0.0.4\n",
    "* *Released*: 2025/07/08\n",
    "* *Author(s)*: Bryan Gee (UT Libraries, University of Texas at Austin; bryan.gee@austin.utexas.edu; ORCID: [0000-0003-4517-3290](https://orcid.org/0000-0003-4517-3290))\n",
    "* *Contributor(s)*: None\n",
    "* *License*: [GNU GPLv3](https://www.gnu.org/licenses/gpl-3.0.en.html)\n",
    "* *README last updated*: 2025/07/08"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b23e9",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This is the Jupyter Notebook version of the *dataverse-file-assessment.py* script. It contains the same functionality but is designed to provide a more detailed explanation of different steps and is targeted at an introductory level of familiarity with Python, APIs, and Dataverse. In theory, it should be accessible to a new institutional liaison for the Texas Data Repository (TDR) within a few weeks of their introduction to TDR as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852cb045",
   "metadata": {},
   "source": [
    "### Modules\n",
    "\n",
    "You will need the following modules; several of these are not included in the default installation of Python and will need to be installed (e.g., through pip). If you need help getting started with how to install packages, please refer to this link: https://packaging.python.org/en/latest/tutorials/installing-packages/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de84e17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from urllib.parse import urlparse, parse_qs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936f050",
   "metadata": {},
   "source": [
    "### Toggles\n",
    "\n",
    "Toggles are Boolean (TRUE/FALSE) variables that define whether part of the workflow should or shouldn't happen or in what fashion it should happen. For example, the *test* toggle will enable or disable 'Test' mode; when 'Test' is set to TRUE, it will cap how many records it retrieves in order to do a really quick run. Depending on the workflow and what you set the cap at, this could cut the runtime by as much as 99% to under a minute. This is ideal for making sure that the workflow as you have (re)configured it will work all the way through, rather than running in non-test mode and getting 80% of the way into an hour-long process, only to discover you have a code-breaking error at the end. There are four toggles in this workflow:\n",
    "\n",
    "* **test**: as noted above, this will enable or disable 'Test' mode. The restricted retrieval caps are defined in the *config.json* file (see next section).\n",
    "* **onlyMyInstitution**: this toggle will define whether to look at only your institution (e.g., 'UT Austin') or all institutions in TDR. As a note, only a superuser will be able to retrieve certain records (e.g., drafts) for all dataverses; institutional liaisons will not be able to get this information for an institution that they don't belong to.\n",
    "* **versionsAPI**: the primary API endpoint that retrieves detailed metadata on individual datasets/files is the Native API. This endpoint only returns the most recent version's metadata. Therefore, if metadata has changed over time, you may not get a complete record of something like total file size (if files have been replaced/deleted) or authorship (if authors have been removed) because everything except the latest version is not retrieved. The versions API will return the same level of detail for all versions, but it is more time-intensive because of this, and in some instances, it may be known or suspected that outdated versions do not contribute significantly (e.g., a dataset that has only been versioned to add metadata like keywords, without file changes). If the toggle is set to 'TRUE,' the script will loop through this endpoint as well.\n",
    "* **excludeDrafts**: related to *onlyMyInstitution*, if you want to ensure standardized results across all institutions and don't have superuser permissions, you may want to toggle this on to export versions of the final outputs that omit unpublished dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "076ed836",
   "metadata": {},
   "outputs": [],
   "source": [
    "#toggle for test environment (incomplete run, faster to complete)\n",
    "test = True\n",
    "#toggle to only look at your/one institution in TDR\n",
    "onlyMyInstitution = True \n",
    "#toggle for stage 3 retrieval\n",
    "versionsAPI = True\n",
    "#toggle for excluding unpublished\n",
    "excludeDrafts = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15714f2",
   "metadata": {},
   "source": [
    "### Dates\n",
    "\n",
    "Both for filenames and in order to calculate how long the script takes, we define two timestamps. These can then be dynamically called through the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc0d6cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting timestamp at start of script to calculate run time\n",
    "startTime = datetime.now() \n",
    "#creating variable with current date for appending to filenames\n",
    "todayDate = datetime.now().strftime(\"%Y%m%d\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aada3f2a",
   "metadata": {},
   "source": [
    "### config file\n",
    "The *config.json* file, which can be variably named and in various formats (e.g., Michael uses a .env text file), provides parameters for the analysis. Externalizing the parameters helps to keep the script clean, since most parameters don't need to be updated regularly, and protects sensitive information like API keys. This file should never be distributed to anyone else; instead, share the *config-template.json* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d67799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in config file\n",
    "with open('config.json', 'r') as file:\n",
    "    config = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375f373",
   "metadata": {},
   "source": [
    "Because you can't \"comment out\" remarks in a JSON file in the same way that you can with Python, I want to provide some comments on what's in the config file below. The first example is showing you what's in the 'INSTITUTION' block, which is various parameters for a specific institution. You will want to modify these in the same syntax for your own institution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "804e118a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'University of Texas at Austin', 'filename': 'UT-austin', 'uniqueIdentifier': 'Austin', 'ror': 'https://ror.org/00hj54h04', 'myInstitution': 'UT Austin'}\n"
     ]
    }
   ],
   "source": [
    "my_institution = config['INSTITUTION']\n",
    "print(my_institution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4df04a",
   "metadata": {},
   "source": [
    "You don't need to load in sequential levels of a nested object; you can go directly to the field you want, as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd244024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String to add to filenames: UT-austin.\n",
      "\n",
      "Short hand version of institution name: UT Austin.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##read in filename version of your institution's name\n",
    "my_institution_filename = config['INSTITUTION']['filename']\n",
    "###condition what goes in the filename based on toggle for which institution(s) to ping\n",
    "if onlyMyInstitution:\n",
    "    institutionFilename = my_institution_filename\n",
    "else:\n",
    "    institutionFilename = \"all-institutions\"\n",
    "##read in short-hand version of your institution's name\n",
    "my_institution_shortName = config[\"INSTITUTION\"][\"myInstitution\"]\n",
    "\n",
    "print(f'String to add to filenames: {my_institution_filename}.\\n')\n",
    "print(f'Short hand version of institution name: {my_institution_shortName}.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9551e1",
   "metadata": {},
   "source": [
    "The 'VARIABLES' field is what dictates the size of the retrieval to be made. Limits on attributes like page size (how many in one call), page count (how many in a total call), and starting parameters will be defined in API documentation: https://guides.dataverse.org/en/latest/api/index.html. Each API is slightly different, so you will need to adapt code based on the API. This script only uses the Dataverse API, but if you need help with the Crossref, DataCite, Dryad, Figshare, OpenAlex, or Zenodo APIs, reach out to Bryan who has code for all of these. For this specific script, you should (be able to) leave these API parameters as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50fe8047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAGE_SIZES': {'datacite_prod': 1000, 'datacite_test': 200, 'dataverse': 200}, 'PAGE_LIMITS': {'datacite_test': 5, 'datacite_prod': 600, 'dspace_test': 3, 'dspace_prod': 100, 'tdr_test': 10, 'tdr_prod': 100}, 'PAGE_STARTS': {'datacite': 0, 'dataverse': 0}, 'PAGE_INCREMENTS': {'dataverse': 1}}\n"
     ]
    }
   ],
   "source": [
    "apiParams = config['VARIABLES']\n",
    "print(apiParams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971f0d08",
   "metadata": {},
   "source": [
    "Not all of the fields (either first-order or nested fields) are used for this script. Some of them pertain only to the other script in this repository (which isn't really that related to be honest). You won't need to bother with the various permutations of your institution's name, for example. Most of the config file is taken up by these dictionaries of key-value pairs or lists of strings. For example, the 'WORDS' field shown below is a series of words that are considered non-descriptive; these are used for a metadata assessment of dataset titles down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10f1a32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'articles': ['a', 'the', 'thee', 'an'], 'conjunctions': ['and', 'or', 'but'], 'prepositions': ['in', 'to', 'of', 'for', 'with', 'as', 'from'], 'auxiliary_verbs': ['is', 'are', 'was', 'were'], 'possessives': ['our', 'my'], 'descriptors': ['data', 'dataset', 'dataverse', 'repository', 'code', 'codes', 'script', 'scripts', 'software', 'supplemental', 'supplementary', 'supporting', 'figure', 'table', 'figures', 'tables', 'file', 'files', 'et al.', 'raw', 'manuscript', 'article', 'journal', 'preprint', 'replication', 'readme', 'github', 'final', 'output'], 'order': ['S1', 'S2', 'S3'], 'version': ['v1', 'v1.0', 'v2', 'v2.0', 'v3', 'v3.0']}\n"
     ]
    }
   ],
   "source": [
    "words = config['WORDS']\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9652828",
   "metadata": {},
   "source": [
    "Several fields are key-value pairs of mimeType file formats and friendly file formats. These are used to systematically assess datasets and files to look for things like whether there is a code file or a README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf2c21fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'application/gzip': 'GZIP (compressed archive)', 'application/x-7z-compressed': '7z (compressed archive)', 'application/x-gzip': 'GZIP (compressed archive)', 'application/x-rar': 'RAR (compressed archive)', 'application/x-tar': 'TAR (compressed archive)', 'application/zip': 'ZIP (compressed archive)', 'TAR/GNU Zip (.tar.gz/.tgz) (application/x-gzip)': 'TAR.GZ (compressed archive)', 'ZIP: PKZIP (application/zip)': 'ZIP (compressed archive)'}\n"
     ]
    }
   ],
   "source": [
    "compressed = config['COMPRESSED_FORMATS']\n",
    "print(compressed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976f155f",
   "metadata": {},
   "source": [
    "**Can you just tell me what I need to do with the *config.json* file?**\n",
    "\n",
    "There are only three things you need to change in this file to tailor it for another institution:\n",
    "\n",
    "* **Add your Dataverse API token**: this is the first field in the config file and should be blank (\"\") in the template. In order to get your token, log into TDR, go to your name at the top right, and click the dropdown arrow - you should see an API token button. Do not share your token!\n",
    "* **Update ['INSTITUTION']['filename']**: this is the string that will automatically be baked into filenames (so that you don't have to manually go through and change many lines of code). This can be whatever you want.\n",
    "* **Update [\"INSTITUTION\"][\"myInstitution\"]**: this is a controlled vocabulary that I cooked up for this workflow. Pick your institution out of the following:\n",
    "\n",
    "  * \"Baylor\"\n",
    "  * \"Houston\"\n",
    "  * \"HSC Fort Worth\"\n",
    "  * \"SMU\"\n",
    "  * \"TAMU\"\n",
    "  * \"TAMU Galveston\"\n",
    "  * \"TAMU International\"\n",
    "  * \"Texas State\"\n",
    "  * \"Texas Tech\"\n",
    "  * \"Texas Women's University\"\n",
    "  * \"UT Arlington\"\n",
    "  * \"UT Austin\"\n",
    "  * \"UT San Antonio Health\"\n",
    "  * \"UT Southwestern Medical\"\n",
    "\n",
    "Specifically for Texas A&M, enter \"TAMU\" to get records from all three TAMU campuses; there is a downstream process to make sure that it pulls from all three dataverses. This process currently includes some institutions that are no longer members of TDR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375798c0",
   "metadata": {},
   "source": [
    "### Remaining set-up\n",
    "\n",
    "The next few blocks handle the rest of the the setting up of the scripting process. The first step is to create some directories. This script is pretty simple, but in order to ensure that files are consistently saved and read from the same places for everyone, the script will first look for certain subdirectories in the folder where this script is contained and make them if they don't exist. As you can see, if the 'Test' toggle is enabled, it will make a *test* subdirectory and change the directory to *test.* So test and non-test outputs are separated to avoid any confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48335b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test directory found - no need to recreate\n",
      "test outputs directory found - no need to recreate\n"
     ]
    }
   ],
   "source": [
    "#creating directories\n",
    "if test:\n",
    "    if os.path.isdir(\"test\"):\n",
    "        print(\"test directory found - no need to recreate\")\n",
    "    else:\n",
    "        os.mkdir(\"test\")\n",
    "        print(\"test directory has been created\")\n",
    "    os.chdir('test')\n",
    "    if os.path.isdir(\"outputs\"):\n",
    "        print(\"test outputs directory found - no need to recreate\")\n",
    "    else:\n",
    "        os.mkdir(\"outputs\")\n",
    "        print(\"test outputs directory has been created\")\n",
    "else:\n",
    "    if os.path.isdir(\"outputs\"):\n",
    "        print(\"outputs directory found - no need to recreate\")\n",
    "    else:\n",
    "        os.mkdir(\"outputs\")\n",
    "        print(\"outputs directory has been created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9761b98",
   "metadata": {},
   "source": [
    "### Utilized API endpoints\n",
    "\n",
    "For maximum coverage, we need to query three different endpoints in the Dataverse API. A comparison of what information is retrieved from these \n",
    "\n",
    "![Comparison of information retrieved from Dataverse endpoints](dataverse-apis-information-comparison.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ddd886",
   "metadata": {},
   "source": [
    "#### Basic API params\n",
    "\n",
    "The next step is basic API parameters. This block defines the first API endpoint (the [Search API](https://guides.dataverse.org/en/latest/api/search.html)). It then dynamically calls in certain parameters about retrieval size limits from the *config.json* file, with different values based on whether you are in the Test mode. The API key is also called in. The value '*k*' is kept in here, instead of the *config.json* file, because it should always be set as zero (i.e. don't touch that). It's used to count pages while the API call is being made. The *query* value is set as the DOI prefix for all TDR deposits - this is because you cannot do a blank search through the API, so you need a value that you know will return 100% of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa421717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning to define API call parameters.\n",
      "Retrieving 200 records per page over 10 pages.\n"
     ]
    }
   ],
   "source": [
    "print(\"Beginning to define API call parameters.\")\n",
    "url_tdr = \"https://dataverse.tdl.org/api/search/\"\n",
    "\n",
    "##set API-specific params\n",
    "###Dataverse\n",
    "page_limit_dataverse = config['VARIABLES']['PAGE_LIMITS']['tdr_test'] if test else config['VARIABLES']['PAGE_LIMITS']['tdr_prod']\n",
    "page_size = config['VARIABLES']['PAGE_SIZES']['dataverse'] if test else config['VARIABLES']['PAGE_SIZES']['dataverse']\n",
    "\n",
    "print(f\"Retrieving {page_size} records per page over {page_limit_dataverse} pages.\")\n",
    "\n",
    "###for TDR, affiliation is not reliable for returning all relevant results; the DOI prefix is used as the most generic common denominator for datasets\n",
    "query = '10.18738/T8/'\n",
    "page_start_dataverse = config['VARIABLES']['PAGE_STARTS']['dataverse']\n",
    "page_increment = config['VARIABLES']['PAGE_INCREMENTS']['dataverse']\n",
    "k = 0\n",
    "\n",
    "headers_tdr = {\n",
    "    'X-Dataverse-key': config['KEYS']['dataverseToken']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470a3aa3",
   "metadata": {},
   "source": [
    "#### Institution-specific parameters\n",
    "\n",
    "The next codeblock is using what's called the 'subtree'; this is a field that contains a unique code for each institution and directs to your institution's specific dataverse. Each institution has its own set of parameters, which are largely constant between them except for the subtree, defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46611990",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_tdr_ut_austin = {\n",
    "    'q': query,\n",
    "    'subtree': 'utexas',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_baylor = {\n",
    "    'q': query,\n",
    "    'subtree': 'baylor',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_smu = {\n",
    "    'q': query,\n",
    "    'subtree': 'smu',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_tamu = {\n",
    "    'q': query,\n",
    "    'subtree': 'tamu',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_txst = {\n",
    "    'q': query,\n",
    "    'subtree': 'txst',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_ttu = {\n",
    "    'q': query,\n",
    "    'subtree': 'ttu',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_houston = {\n",
    "    'q': query,\n",
    "    'subtree': 'uh',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_hscfw = {\n",
    "    'q': query,\n",
    "    'subtree': 'unthsc',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_tamug = {\n",
    "    'q': query,\n",
    "    'subtree': 'tamug',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "params_tdr_tamui = {\n",
    "    'q': query,\n",
    "    'subtree': 'tamiu',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "params_tdr_utsah = {\n",
    "    'q': query,\n",
    "    'subtree': 'uthscsa',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "params_tdr_utswm = {\n",
    "    'q': query,\n",
    "    'subtree': 'utswmed',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_uta = {\n",
    "    'q': query,\n",
    "    'subtree': 'uta',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}\n",
    "\n",
    "params_tdr_twu = {\n",
    "    'q': query,\n",
    "    'subtree': 'twu',\n",
    "    'type': 'dataset',\n",
    "    'start': page_start_dataverse,\n",
    "    'page': page_increment,\n",
    "    'per_page': page_limit_dataverse\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5c955a",
   "metadata": {},
   "source": [
    "#### Parameter combinations\n",
    "\n",
    "The next codeblock is setting up the code to handle different conditions based on whether you want to look at only your institution or all TDR institutions. The *all_params* object combines all of the institution-specific parameters. The *tamu_combined_params* object combines only TAMU parameters. In theory, you can cook up any combination you want (e.g., UT system schools)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "459607bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params = {\n",
    "        \"UT Austin\": params_tdr_ut_austin,\n",
    "        \"Baylor\": params_tdr_baylor,\n",
    "        \"SMU\": params_tdr_smu,\n",
    "        \"TAMU\": params_tdr_tamu,\n",
    "        \"Texas State\": params_tdr_txst,\n",
    "        \"Texas Tech\": params_tdr_ttu,\n",
    "        \"Houston\": params_tdr_houston,\n",
    "        \"HSC Fort Worth\": params_tdr_hscfw,\n",
    "        \"TAMU Galveston\": params_tdr_tamug,\n",
    "        \"TAMU International\": params_tdr_tamui,\n",
    "        \"UT San Antonio Health\": params_tdr_utsah,\n",
    "        \"UT Southwestern Medical\": params_tdr_utswm,\n",
    "        \"UT Arlington\": params_tdr_uta,\n",
    "        \"Texas Women's University\": params_tdr_twu\n",
    "    }\n",
    "\n",
    "tamu_combined_params = {\n",
    "        \"TAMU\": params_tdr_tamu,\n",
    "        \"TAMU Galveston\": params_tdr_tamug,\n",
    "        \"TAMU International\": params_tdr_tamui\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f6e76b",
   "metadata": {},
   "source": [
    "The next codebook then uses 'if-else' statements to tell the script which combination (or single-institution parameter set) to use. The script is basically saying:\n",
    "\n",
    "* if you set the *onlyMyInstitution* toggle to TRUE, AND you set the *my_institution_shortName* to \"TAMU,\" then I will use the combined TAMU parameter set (*tamu_combined_params*).\n",
    "* if you set the *onlyMyInstitution* toggle to TRUE, BUT you set the *my_institution_shortName* to anything other than \"TAMU,\" then I will use the single institution parameter set (e.g., *params_tdr_ut_austin* for UT Austin).\n",
    "* if you set the *onlyMyInstitution* toggle to FALSE, then I will search across all institutions with *all_params*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a0ef4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#substitute for your institution\n",
    "if onlyMyInstitution:\n",
    "    if my_institution_shortName == \"TAMU\":\n",
    "        params_list = tamu_combined_params\n",
    "    else:\n",
    "        params_list = {\n",
    "            my_institution_shortName: all_params[my_institution_shortName]\n",
    "        }\n",
    "else:\n",
    "    params_list = all_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b1d1df",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Functions are used when you need to apply the same logic many times throughout a script; it can make the process more concise rather than having to repeat tens to hundreds of lines of code. To be honest, the functions below are pretty technical and are the result of a lot of experimentation with many APIs. You should not touch them unless you know what you are doing, but feel free to ask questions if you want to know what a given line is doing. As a note, even though some functions' names might suggest they are designed only for querying all institutions (e.g., *retrieve_all_data_for_institutions*), if you are only running this for a single institution or a subset of institutions, it will still work the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40018b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "##function to get single page from Dataverse API\n",
    "def retrieve_page_dataverse(url, params=None, headers=None):\n",
    "    try:\n",
    "        response = requests.get(url, params=params, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error retrieving page: {e}\")\n",
    "        return {'data': {'items': [], 'total_count': 0}}\n",
    "##function to get many pages from Dataverse API\n",
    "def retrieve_all_data_dataverse(url, params, headers):\n",
    "    global page_limit_dataverse, k\n",
    "    # create empty list\n",
    "    all_data_tdr = []\n",
    "\n",
    "    while True and k < page_limit_dataverse: \n",
    "        k+=1\n",
    "        data = retrieve_page_dataverse(url, params, headers)  \n",
    "        total_count = data['data']['total_count']\n",
    "        total_pages = math.ceil(total_count/page_limit_dataverse)\n",
    "        print(f\"Retrieving Page {params['page']} of {total_pages} pages...\\n\")\n",
    "\n",
    "        if not data['data']:\n",
    "            print(\"No data found.\")\n",
    "            break\n",
    "    \n",
    "        all_data_tdr.extend(data['data']['items'])\n",
    "        \n",
    "        #update pagination\n",
    "        params['start'] += page_limit_dataverse\n",
    "        params['page'] += 1\n",
    "        \n",
    "        if params['start'] >= total_count:\n",
    "            print(\"End of response.\")\n",
    "            break\n",
    "\n",
    "    return all_data_tdr\n",
    "\n",
    "##function to retrieve many pages from many institutions\n",
    "def retrieve_all_data_for_institutions(url, params_list, headers):\n",
    "    all_data = []\n",
    "\n",
    "    for institution_name, params in params_list.items():\n",
    "        global page_limit_dataverse, k\n",
    "        k = 0  #reset k for each institution\n",
    "\n",
    "        all_data_tdr = retrieve_all_data_dataverse(url, params, headers)\n",
    "        for entry in all_data_tdr:\n",
    "            entry['institution'] = institution_name \n",
    "            all_data.append(entry)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "\n",
    "##function to count descriptive words\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    total_words = len(words)\n",
    "    descriptive_count = sum(1 for word in words if word not in nondescriptive_words)\n",
    "    return total_words, descriptive_count\n",
    "\n",
    "##function to account for when a single word may or may not be descriptive but is certainly uninformative if in a certain combination\n",
    "def adjust_descriptive_count(row):\n",
    "    if ('supplemental material' in row['titleReformatted'].lower() or\n",
    "            'supplementary material' in row['titleReformatted'].lower() or\n",
    "            'supplementary materials' in row['titleReformatted'].lower() or\n",
    "            'supplemental materials' in row['titleReformatted'].lower()):\n",
    "        return max(0, row['descriptiveWordCount_title'] - 1)\n",
    "    return row['descriptiveWordCount_title']\n",
    "\n",
    "##function to assign size bins (file or dataset level)\n",
    "def assign_size_bins(df, column='fileSize', new_column='fileSizeBin'):\n",
    "    df=df.copy()\n",
    "    bins = [\n",
    "        (0, 1 * 1024, \"0-10 kB\"),\n",
    "        (1 * 1024, 1 * 1024 * 1024, \"10 kB-1 MB\"),\n",
    "        (1 * 1024 * 1024, 100 * 1024 * 1024, \"1-100 MB\"),\n",
    "        (100 * 1024 * 1024, 1 * 1024 * 1024 * 1024, \"100 MB-1 GB\"),\n",
    "        (1 * 1024 * 1024 * 1024, 10 * 1024 * 1024 * 1024, \"1-10 GB\"),\n",
    "        (10 * 1024 * 1024 * 1024, 15 * 1024 * 1024 * 1024, \"10-15 GB\"),\n",
    "        (15 * 1024 * 1024 * 1024, 20 * 1024 * 1024 * 1024, \"15-20 GB\"),\n",
    "        (20 * 1024 * 1024 * 1024, 25 * 1024 * 1024 * 1024, \"20-25 GB\"),\n",
    "        (25 * 1024 * 1024 * 1024, 30 * 1024 * 1024 * 1024, \"25-30 GB\"),\n",
    "        (30 * 1024 * 1024 * 1024, 40 * 1024 * 1024 * 1024, \"30-40 GB\"),\n",
    "        (40 * 1024 * 1024 * 1024, 50 * 1024 * 1024 * 1024, \"40-50 GB\"),\n",
    "    ]\n",
    "\n",
    "    #default set to empty\n",
    "    df[new_column] = \"Empty\"\n",
    "    for lower, upper, label in bins:\n",
    "        df.loc[(df[column] > lower) & (df[column] <= upper), new_column] = label\n",
    "    #maximum bin\n",
    "    df.loc[df[column] > 50 * 1024 * 1024 * 1024, new_column] = \">50 GB\"\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753cd0ca",
   "metadata": {},
   "source": [
    "## Retrieval process: part 1\n",
    "\n",
    "Because we defined a series of functions related to retrieving records from the Dataverse API, potentially across all institutions, the process to trigger the API call is a single line, as seen below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba4c6459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TDR retrieval.\n",
      "\n",
      "Retrieving Page 1 of 150 pages...\n",
      "\n",
      "Retrieving Page 2 of 150 pages...\n",
      "\n",
      "Retrieving Page 3 of 150 pages...\n",
      "\n",
      "Retrieving Page 4 of 150 pages...\n",
      "\n",
      "Retrieving Page 5 of 150 pages...\n",
      "\n",
      "Retrieving Page 6 of 150 pages...\n",
      "\n",
      "Retrieving Page 7 of 150 pages...\n",
      "\n",
      "Retrieving Page 8 of 150 pages...\n",
      "\n",
      "Retrieving Page 9 of 150 pages...\n",
      "\n",
      "Retrieving Page 10 of 150 pages...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting TDR retrieval.\\n\")\n",
    "all_data = retrieve_all_data_for_institutions(url_tdr, params_list, headers_tdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d3c88e",
   "metadata": {},
   "source": [
    "### Subsetting API response\n",
    "\n",
    "After the call, the API response is cached in the system but not saved in any form. You could, if you wanted to, export it as a JSON file, but I prefer working in CSV / tabular format for a lot of this work, so we'll convert it to a dataframe first. The first step in that is selecting certain fields that we want; not all fields are useful. \n",
    "\n",
    "**Why not just explode each JSON field into a separate Excel column (e.g., *authors* becomes *authors_1*, *authors_2*, etc.)?**\n",
    "\n",
    "I actually used to do this when I was developing another Python workflow (https://github.com/utlibraries/research-data-discovery). What I discovered is that, given any appreciable number of dataset records (let's say more than 250), at least one is virtually guaranteed to have an inordinate number of nested fields (e.g., 29 authors). This means that the script will flatten just that one field into 29 columns, which takes a lot longer to do and a lot longer to work through. If you start getting up above 1,000 records, you might not have enough RAM to actually do this. When we know what fields are important, we can just subset for them.\n",
    "\n",
    "**Why are we subsetting so few fields?** \n",
    "\n",
    "The Search API endpoint returns relatively little metadata because it's intended for a broad capture (one call, many results). It doesn't even return information like author affiliations. So the main purpose of this call is just to get every single dataset of interest, and then we'll pass the DOIs into a different API endpoint to get more detailed metadata.\n",
    "\n",
    "The way that this first subsetting process works is to create an empty list to hold the subsetted output (*data_select_tdr*) and then to loop through the full API response (*all_data*), pulling out specific fields. The syntax (e.g., *item.get*) is based on the hierarchy of the output. You'll see in later processes how we'll need to extract more nested objects. The script then appends each dataset's subsetted metadata into the list, and we convert it to a dataframe with the *pandas* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6f50cfe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting TDR filtering.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting TDR filtering.\\n\")\n",
    "data_select_tdr = []\n",
    "for item in all_data:\n",
    "    id = item.get('global_id', '')\n",
    "    type = item.get('type', '')\n",
    "    institution = item.get('institution','')\n",
    "    status = item.get('versionState', '')\n",
    "    name = item.get('name', '')\n",
    "    dataverse = item.get('name_of_dataverse', '')\n",
    "    majorV = item.get('majorVersion', 0)\n",
    "    minorV = item.get('minorVersion', 0)\n",
    "    comboV = f\"{majorV}.{minorV}\"\n",
    "    data_select_tdr.append({\n",
    "        'institution': institution, \n",
    "        'doi': id,\n",
    "        'type': type,\n",
    "        'status': status,\n",
    "        'title': name,\n",
    "        'dataverse': dataverse,\n",
    "        'majorVersion': majorV,\n",
    "        'minorVersion': minorV,\n",
    "        'totalVersion': comboV\n",
    "    })\n",
    "\n",
    "df_data_select_tdr = pd.DataFrame(data_select_tdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e0a392",
   "metadata": {},
   "source": [
    "#### Post-conversion cleaning\n",
    "\n",
    "We'll do a little bit of cleaning / modification of the data now that they're in a dataframe. The first step (filtering for 'type' = 'dataset') is redundant now because the 'type' is specified in the API call (this omits retrieval of 'dataverse' and 'file' records), but it's retained because it doesn't take up any more time and can be useful if you change the parameters later. The two other steps edit the DOI field to remove a 'doi:' string that always precedes the DOI value in that column and to add a Boolean column indicating whether a dataset has been versioned or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1621bd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dataverses and files\n",
    "filtered_tdr = df_data_select_tdr[df_data_select_tdr['type'] == 'dataset']\n",
    "#editing DOI field\n",
    "filtered_tdr['doi'] = filtered_tdr['doi'].str.replace('doi:', '')\n",
    "#add column for versioned\n",
    "filtered_tdr['versioned'] = filtered_tdr.apply(lambda row: 'Versioned' if (row['majorVersion'] > 1) or (row['minorVersion'] > 0) else 'Not versioned', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3856a8d",
   "metadata": {},
   "source": [
    "### Metadata assessment: part 1\n",
    "\n",
    "We can't really do too much metadata assessment at the dataset/file level because of the limited metadata we get from the Search API. We can do one though: how descriptive are dataset titles? This pulls in a list of nondescriptive words from the *config.json* file (you can modify these if you wish) and then looks through the title to count how many other words remain. This operates on a few combinations of words as well (*adjust_descriptive_count* function) where the individual words aren't necessarily nondescriptive in isolation but are when combined (e.g., 'supplemental information'). Titles are formatted to remove underscores and hyphens, otherwise the entire string gets treated as one word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ab339fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata assessments\n",
    "##title\n",
    "##assess 'descriptiveness of dataset title'\n",
    "words = config['WORDS']\n",
    "###add integers\n",
    "numbers = list(map(str, range(1, 1000000)))\n",
    "###combine all into a single set\n",
    "nondescriptive_words = set(\n",
    "    words['articles'] +\n",
    "    words['conjunctions'] +\n",
    "    words['prepositions'] +\n",
    "    words['auxiliary_verbs'] +\n",
    "    words['possessives'] +\n",
    "    words['descriptors'] +\n",
    "    words['order'] +\n",
    "    words['version'] +\n",
    "    numbers\n",
    ")\n",
    "\n",
    "filtered_tdr['titleReformatted'] = filtered_tdr['title'].str.replace('_', ' ') \n",
    "filtered_tdr['titleReformatted'] = filtered_tdr['title'].str.replace('-', ' ') #gets around text linked by underscores counting as 1 word\n",
    "filtered_tdr['titleReformatted'] = filtered_tdr['titleReformatted'].str.lower()\n",
    "filtered_tdr[['totalWordCount_title', 'descriptiveWordCount_title']] = filtered_tdr['titleReformatted'].apply(lambda x: pd.Series(count_words(x)))\n",
    "\n",
    "filtered_tdr['descriptiveWordCount_title'] = filtered_tdr.apply(adjust_descriptive_count, axis=1)\n",
    "filtered_tdr['nondescriptiveWordCount_title'] = filtered_tdr['totalWordCount_title'] - filtered_tdr['descriptiveWordCount_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf64d3b",
   "metadata": {},
   "source": [
    "## Retrieval process: part 2\n",
    "\n",
    "The next part of the process is where we get richer metadata. In this step, we go through the [Native API](https://guides.dataverse.org/en/latest/api/native-api.html). This endpoint requires you to feed a single DOI into it, and then it returns very detailed, file-level metadata for the latest version of the dataset (regardless of publication status). \n",
    "\n",
    "First, however, we need to deal with a quirk of the Search API: that it returns two records for any dataset that has been previously published and that is now in draft status. These records could be functionally identical (e.g., author accidentally puts published dataset into draft mode, strands it there without making changes) or quite significantly different. The code below de-duplicates these records, retaining only the published version, but saves a CSV file with a list of pairs of records with this status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef508fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total datasets to be analyzed: 99.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#sort on status, setting 'DRAFT' at bottom to remove this version for published datasets that are in draft state, retain entry of 'PUBLISHED'\n",
    "filtered_tdr = filtered_tdr.sort_values(by='status', ascending=False)\n",
    "filtered_tdr.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-deposits.csv\")\n",
    "filtered_tdr_deduplicated = filtered_tdr.drop_duplicates(subset=['doi'], keep=\"first\")\n",
    "filtered_tdr_deduplicated.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-deposits-deduplicated.csv\")\n",
    "print(f'Total datasets to be analyzed: {len(filtered_tdr_deduplicated)}.\\n')\n",
    "\n",
    "#create df of published datasets with draft version (retains both entries)\n",
    "commonColumns = ['doi', 'title']\n",
    "duplicates = filtered_tdr.duplicated(subset=commonColumns, keep=False)\n",
    "dualStatusDatasets = filtered_tdr[duplicates]\n",
    "dualStatusDatasets.to_csv(f\"outputs/{todayDate}_{institutionFilename}_dual-status-datasets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e04ebd1",
   "metadata": {},
   "source": [
    "Now we have a list of unique DOIs that we want more metadata on. The following code makes a similar API call to the first one, but through the Native API endpoint. So we define the endpoint (it's a different URL) and then set up a 'for loop,' where we go through each DOI in the de-duplicated dataframe, get its metadata, and save that in an empty list called *results*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9925f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieving additional metadata for deposits by individual API call (one per DOI)\n",
    "##retrieves both published and never-published draft datasets; if a published dataset is currently in DRAFT state, it will return the information for the DRAFT state\n",
    "print(\"Starting Native API call\")\n",
    "url_tdr_native = \"https://dataverse.tdl.org/api/datasets/\"\n",
    "\n",
    "results = []\n",
    "for doi in filtered_tdr_deduplicated['doi']:\n",
    "    try:\n",
    "        response = requests.get(f'{url_tdr_native}:persistentId/?persistentId=doi:{doi}', headers=headers_tdr, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"Retrieving {doi}\\n\")\n",
    "            results.append(response.json())\n",
    "        else:\n",
    "            print(f\"Error retrieving {doi}: {response.status_code}, {response.text}\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Timeout error on DOI {doi}: {e}\")\n",
    "\n",
    "data_tdr_native = {\n",
    "    'datasets': results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13b86bf",
   "metadata": {},
   "source": [
    "Then we do a similar subsetting process to what we did for the first API call. We make an empty list to store the output (*data_select_tdr_native*) and then loop through. You might notice that there are various *X.get* commands in the script below. This is based on the nesting of different fields. For example, *latest* is nested within *data*, so in order to get anything that is nested under *latest*, we need to define that one first. The only way to know how to structure this is to look at an actual API response. You'll see that we're subsetting a lot more fields and getting a lot more detail in the metadata. This code specifically creates an entry for each file listed in a dataset record, so some dataset-level metadata is duplicated across several rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5257e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning dataframe subsetting\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Beginning dataframe subsetting\\n\")\n",
    "data_select_tdr_native = [] \n",
    "for item in data_tdr_native['datasets']:\n",
    "    data = item.get('data', '')\n",
    "    datasetID = data.get('id', '')\n",
    "    pubDate = data.get('publicationDate', '')\n",
    "    latest = data.get('latestVersion', {})\n",
    "    status = latest.get('versionState', '')\n",
    "    status2 = latest.get('latestVersionPublishingState', '')\n",
    "    doi = latest.get('datasetPersistentId', '')\n",
    "    updateDate = latest.get('lastUpdateTime', '')\n",
    "    createDate = latest.get('createTime', '')\n",
    "    releaseDate = latest.get('releaseTime', '')\n",
    "    license = latest.get('license', {})\n",
    "    licenseName = license.get('name', None)\n",
    "    terms = latest.get('termsOfUse', None)\n",
    "    usage = licenseName if licenseName is not None else terms\n",
    "    files = latest.get('files', [])\n",
    "    citation = latest.get('metadataBlocks', {}).get('citation', {})\n",
    "    fields = citation.get('fields', [])\n",
    "    grantAgencies = []\n",
    "    for field in fields:\n",
    "        if field['typeName'] == 'grantNumber':\n",
    "            for grant in field.get('value', []):\n",
    "                grant_number_agency = grant.get('grantNumberAgency', {}).get('value', '')\n",
    "                grantAgencies.append(grant_number_agency)\n",
    "    total_filesize = 0\n",
    "    unique_content_types = set()\n",
    "    fileCount = len(files)\n",
    "    for file_info in files:\n",
    "        file_data = file_info['dataFile']\n",
    "        unique_content_types.add(file_data['contentType'])\n",
    "        file_entry = {\n",
    "            'datasetID': datasetID,\n",
    "            'doi': doi,\n",
    "            #'status': status,\n",
    "            'currentStatus': status2,\n",
    "            'reuseRequirements': usage,\n",
    "            #'fileCount': fileCount,\n",
    "            #'unique_content_types': list(unique_content_types),\n",
    "            'fileID': file_data.get('id', ''),\n",
    "            'public': file_data.get('restricted', ''),\n",
    "            'filename': file_data.get('filename', ''),\n",
    "            'mimeType': file_data.get('contentType', ''),\n",
    "            'friendlyType': file_data.get('friendlyType', ''),\n",
    "            'tabular': file_data.get('tabularData', ''),\n",
    "            'fileSize': file_data.get('filesize', 0),\n",
    "            'storageIdentifier': file_data.get('storageIdentifier', ''),\n",
    "            'creationDate': file_data.get('creationDate', ''),\n",
    "            'publicationDate': file_data.get('publicationDate', '')\n",
    "        }\n",
    "        data_select_tdr_native.append(file_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41b1f57",
   "metadata": {},
   "source": [
    "We're also going to create a different subsetted output from the same API response, this one for individual authors (you can create as many subsetted outputs as you want from a single API response). This one creates a separate entry for each author listed in a dataset record, so some dataset-level metadata is duplicated across several rows. This one has to deal with some extra complexity in how Dataverse structures entries that have ROR IDs vs. ones that don't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72d394d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting dataframe with entries for individual authors\n",
    "author_entries = []\n",
    "for item in data_tdr_native['datasets']:\n",
    "    data = item.get('data', {})\n",
    "    latest = data.get('latestVersion', {})\n",
    "    doi = latest.get('datasetPersistentId', '')\n",
    "    citation = latest.get('metadataBlocks', {}).get('citation', {})\n",
    "    status2 = latest.get('latestVersionPublishingState', '')\n",
    "    fields = citation.get('fields', [])\n",
    "    for field in fields:\n",
    "        if field['typeName'] == 'author':\n",
    "            for author in field.get('value', []):\n",
    "                name = author.get('authorName', {}).get('value', '')\n",
    "                affiliation = author.get('authorAffiliation', {}).get('value', '')\n",
    "                identifier = author.get('authorIdentifier', {}).get('value', '')\n",
    "                scheme = author.get('authorIdentifierScheme', {}).get('value', '')\n",
    "                affiliation_expanded = author.get('authorAffiliation', {}).get('expandedvalue', {}).get('termName', '')\n",
    "                identifier_expanded = author.get('authorIdentifier', {}).get('expandedvalue', {}).get('@id', '')\n",
    "\n",
    "                affiliationName = affiliation_expanded if affiliation_expanded else affiliation\n",
    "                affiliation_ror = affiliation if affiliation_expanded else None\n",
    "\n",
    "                author_entry = {\n",
    "                    'doi': doi,\n",
    "                    'currentStatus': status2,\n",
    "                    'authorName': name,\n",
    "                    'authorAffiliation': affiliationName,\n",
    "                    'rorID': affiliation_ror,\n",
    "                    'authorIdentifier': identifier,\n",
    "                    'authorIdentifierExpanded': identifier_expanded,\n",
    "                    'authorIdentifierScheme': scheme\n",
    "                }\n",
    "                author_entries.append(author_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca93d267",
   "metadata": {},
   "source": [
    "### Initial cleaning\n",
    "\n",
    "We then convert these subsetted responses to dataframes, standardize how the DOIs are listed, and add a column for the file-level output to create a column with just the year the file was created (from a full date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97824c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_tdr_native = pd.json_normalize(data_select_tdr_native)\n",
    "df_author_entries = pd.json_normalize(author_entries)\n",
    "df_select_tdr_native['doi'] = df_select_tdr_native['doi'].str.replace('doi:', '')\n",
    "df_author_entries['doi'] = df_author_entries['doi'].str.replace('doi:', '')\n",
    "df_select_tdr_native['creationDate'] = pd.to_datetime(df_select_tdr_native['creationDate'])\n",
    "df_select_tdr_native['fileCreationYear'] = df_select_tdr_native['creationDate'].dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffb5fde",
   "metadata": {},
   "source": [
    "### Metadata assessment: part 2\n",
    "\n",
    "We do another metadata assessment at this point: binning files by their size (*size_bin*). This dataframe is then merged back with the dataframe from the Search API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b73f4f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select_tdr_native = assign_size_bins(df_select_tdr_native, column='fileSize', new_column='fileSizeBin')\n",
    "df_select_concatenated = pd.merge(filtered_tdr_deduplicated, df_select_tdr_native, on='doi', how=\"left\")\n",
    "df_select_concatenated_exist = df_select_concatenated.dropna(subset=['datasetID']).copy() #removes deaccessioned\n",
    "df_select_concatenated_exist['datasetID'] = df_select_concatenated_exist['datasetID'].astype(int)\n",
    "df_select_concatenated_exist.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-deposits-deduplicated_expanded-metadata.csv\")\n",
    "\n",
    "#subset to datasets that are less than version 2.0 (no major update, no file additions)\n",
    "df_select_concatenated_exist_majorVersion = df_select_concatenated_exist[df_select_concatenated_exist['majorVersion'] > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d865662b",
   "metadata": {},
   "source": [
    "### Retrieval process: part 3\n",
    "\n",
    "This final API call is optional (*versionsAPI* toggle). If you set that to FALSE, this entire codeblock will be skipped. If you run it, the process and metadata are nearly identical to that of the previous Native API call; the [Versions](https://guides.dataverse.org/en/latest/api/native-api.html) endpoint is technically part of the Native API, but it works a little differently. Firstly, this endpoint is public, so it will not return any metadata on drafts of any form (drafts of previously published, drafts of never published). Secondly, it returns metadata on all published versions of a dataset (the default Native endpoint only returns the most recent version). Whether the information of older published versions will significantly impact the results is a personal decision, and you may wish to experiment with it. The resultant output is subsetted and de-duplicated to handle predicted redundancy (e.g., a file present in versions 1 through 3 will have three entries), has the dataset size bin classification applied, and then aligns the columns to be the same as that of the previous outputs. This is done for both file-level output and author-level output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319a891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to use Version endpoint to get info on published version of published datasets that are currently in DRAFT status and all published versions of a dataset with multiple PUBLISHED versions. This endpoint is public and does not return any DRAFTs.\n",
    "#remove datasets that have never been published (will not return any info for this endpoint)\n",
    "df_select_concatenated_exist_published = df_select_concatenated_exist_majorVersion[df_select_concatenated_exist_majorVersion['publicationDate'].notnull()]\n",
    "#deduplicate on datasetID\n",
    "df_select_concatenated_exist_published_dedup = df_select_concatenated_exist_published.drop_duplicates(subset=\"datasetID\", keep=\"first\")\n",
    "\n",
    "if versionsAPI:\n",
    "    results_versions = []\n",
    "    print(\"Beginning Version API query\\n\")\n",
    "    for datasetID in df_select_concatenated_exist_published_dedup['datasetID']:\n",
    "        try:\n",
    "            response = requests.get(f'{url_tdr_native}{datasetID}/versions')\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Retrieving versions of dataset #{datasetID}\")\n",
    "                print()\n",
    "                results_versions.append(response.json())\n",
    "            else:\n",
    "                print(f\"Error retrieving dataset #{datasetID}: {response.status_code}, {response.text}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Timeout error on DOI {doi}: {e}\")\n",
    "\n",
    "    data_tdr_versions = {\n",
    "        'datasets': results_versions\n",
    "    }\n",
    "    print(\"Beginning dataframe subsetting\\n\")\n",
    "    data_select_tdr_versions = [] \n",
    "    for dataset in data_tdr_versions['datasets']:\n",
    "        data = dataset.get('data', [])\n",
    "        for item in data:\n",
    "            doi = item.get('datasetPersistentId', '')\n",
    "            id = item.get(\"id\", '')\n",
    "            datasetid = item.get('datasetId', '')\n",
    "            majorV = str(item.get('versionNumber', 0))\n",
    "            minorV = str(item.get('versionMinorNumber', 0))\n",
    "            status2 = latest.get('latestVersionPublishingState', '')\n",
    "            comboV = f\"{majorV}.{minorV}\"\n",
    "            status = item.get('versionState', '')\n",
    "            for file in item.get('files', []):\n",
    "                fileInfo = file['dataFile']\n",
    "                data_select_tdr_versions.append({\n",
    "                    'doi': doi,\n",
    "                    'versionID': id,\n",
    "                    'datasetID': datasetid,\n",
    "                    #'majorVersion': majorV,\n",
    "                    #'minorVersion': minorV,\n",
    "                    'totalVersion': comboV,\n",
    "                    'fileID': fileInfo.get('id', ''),\n",
    "                    'filename': fileInfo.get('filename', ''),\n",
    "                    'mimeType': fileInfo.get('contentType', ''),\n",
    "                    'friendlyType': fileInfo.get('friendlyType', ''),\n",
    "                    #'status': status,\n",
    "                    'currentStatus': status2,\n",
    "                    #'tabular': fileInfo.get('tabularData', ''),\n",
    "                    'fileSize': fileInfo.get('filesize', ''),\n",
    "                    'storageIdentifier': fileInfo.get('storageIdentifier', ''),\n",
    "                    #'md5': fileInfo.get('md5', ''),\n",
    "                    'creationDate': fileInfo.get('creationDate', ''),\n",
    "                    'publicationDate': fileInfo.get('publicationDate', '')\n",
    "                })\n",
    "    #getting dataframe with entries for individual authors\n",
    "    author_entries_versions = []\n",
    "    for dataset in data_tdr_versions['datasets']:\n",
    "        data = dataset.get('data', [])\n",
    "        for item in data:\n",
    "            doi = item.get('datasetPersistentId', '')\n",
    "            id = item.get(\"id\", '')\n",
    "            status2 = item.get('latestVersionPublishingState', '')\n",
    "            datasetid = item.get('datasetId', '')\n",
    "            citation = item.get('metadataBlocks', {}).get('citation', {})\n",
    "            fields = citation.get('fields', [])\n",
    "            for field in fields:\n",
    "                if field['typeName'] == 'author':\n",
    "                    for author in field.get('value', []):\n",
    "                        name = author.get('authorName', {}).get('value', '')\n",
    "                        affiliation = author.get('authorAffiliation', {}).get('value', '')\n",
    "                        identifier = author.get('authorIdentifier', {}).get('value', '')\n",
    "                        scheme = author.get('authorIdentifierScheme', {}).get('value', '')\n",
    "                        affiliation_expanded = author.get('authorAffiliation', {}).get('expandedvalue', {}).get('termName', '')\n",
    "                        identifier_expanded = author.get('authorIdentifier', {}).get('expandedvalue', {}).get('@id', '')\n",
    "\n",
    "                        affiliationName = affiliation_expanded if affiliation_expanded else affiliation\n",
    "                        affiliation_ror = affiliation if affiliation_expanded else None\n",
    "\n",
    "                        author_entry = {\n",
    "                            'doi': doi,\n",
    "                            'currentStatus': status2,\n",
    "                            'authorName': name,\n",
    "                            'authorAffiliation': affiliationName,\n",
    "                            'rorID': affiliation_ror,\n",
    "                            'authorIdentifier': identifier,\n",
    "                            'authorIdentifierExpanded': identifier_expanded,\n",
    "                            'authorIdentifierScheme': scheme\n",
    "                        }\n",
    "                        author_entries_versions.append(author_entry)\n",
    "\n",
    "    df_select_tdr_versions = pd.json_normalize(data_select_tdr_versions)\n",
    "    df_author_entries_versions = pd.json_normalize(author_entries_versions)\n",
    "    df_select_tdr_versions['doi'] = df_select_tdr_versions['doi'].str.replace('doi:', '')\n",
    "    df_author_entries_versions['doi'] = df_author_entries_versions['doi'].str.replace('doi:', '')\n",
    "    #removing duplicate entries for a given file that has not changed across multiple versions\n",
    "    df_select_tdr_versions['totalVersion'] = df_select_tdr_versions['totalVersion'].astype(float)\n",
    "    df_select_tdr_versions = df_select_tdr_versions.sort_values(by='totalVersion')\n",
    "    df_select_tdr_versions_deduplicated = df_select_tdr_versions.drop_duplicates(subset=['datasetID', 'storageIdentifier'], keep='first')\n",
    "\n",
    "    df_select_tdr_versions_deduplicated = assign_size_bins(df_select_tdr_versions_deduplicated, column='fileSize', new_column='fileSizeBin')\n",
    "\n",
    "    df_select_versions_concatenated_released = pd.merge(df_select_tdr_versions_deduplicated, filtered_tdr_deduplicated, on='doi', how=\"left\")\n",
    "\n",
    "    #pruning and renaming columns in the two dataframes that collectively (should) have all of the files (from the Native and the Version endpoints)\n",
    "    df_version_pruned = df_select_versions_concatenated_released[[\"versionID\", \"datasetID\", \"totalVersion_x\", \"filename\", \"fileID\", \"mimeType\", \"friendlyType\", \"fileSize\", \"storageIdentifier\", \"creationDate\", \"publicationDate\", \"institution\", \"doi\", \"fileSizeBin\", \"title\", \"dataverse\"]]\n",
    "    df_version_pruned = df_version_pruned.rename(columns={'totalVersion_x': 'totalVersion', 'filename_x': 'filename', 'fileSize_x': 'fileSize', 'storageIdentifier_x': 'storageIdentifier', 'creationDate_x': 'creationDate', 'publicationDate_x':'publicationDate'})\n",
    "    df_version_pruned['creationYear'] = pd.to_datetime(df_version_pruned['creationDate'], format=\"%Y-%m-%d\").dt.year\n",
    "    df_version_pruned['publicationYear'] = pd.to_datetime(df_version_pruned['publicationDate'], format=\"%Y-%m-%d\").dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bacdea3",
   "metadata": {},
   "source": [
    "The following block of code is run regardless of whether you queried the Versions endpoint or not. It standardizes some columns and creates some additional date columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b07f1fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_native_pruned = df_select_concatenated_exist[[\"datasetID\", \"totalVersion\", \"filename\", \"fileID\", \"mimeType\", \"friendlyType\", \"fileSize\", \"storageIdentifier\", \"creationDate\", \"publicationDate\", \"institution\", \"doi\", \"fileSizeBin\", \"title\", \"dataverse\"]]\n",
    "df_native_pruned = df_native_pruned.copy()\n",
    "df_native_pruned['creationYear'] = pd.to_datetime(df_native_pruned['creationDate'], format=\"%Y-%m-%dT%H:%M:%SZ\").dt.year\n",
    "df_native_pruned['publicationYear'] = pd.to_datetime(df_native_pruned['publicationDate'], format=\"%Y-%m-%d\").dt.year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd9ee23",
   "metadata": {},
   "source": [
    "### Conditional concatenation\n",
    "\n",
    "If you ran the Versions endpoint process, you will need to combine that with the first Native API output and de-duplicate. This block of code uses an 'if-else' statement to either combine the two dataframes and then de-duplicate them (both file-level and author-level), or it just does the de-duplication process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b760328c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if versionsAPI:\n",
    "    df_all_files_concat = pd.concat([df_version_pruned, df_native_pruned], ignore_index=True)\n",
    "    df_all_files_concat = df_all_files_concat.rename(columns={'title': 'datasetTitle'})\n",
    "\n",
    "    #deduplicate\n",
    "    ##create fake versionID for drafts to ensure proper sorting and deduplicating\n",
    "    df_all_files_concat['versionID'] = df_all_files_concat['versionID'].fillna(9999999)\n",
    "    df_all_files_concat = df_all_files_concat.sort_values(by='versionID')\n",
    "    df_all_files_concat_deduplicated = df_all_files_concat.drop_duplicates(subset=['storageIdentifier'], keep='first')\n",
    "    df_all_files_concat_deduplicated = df_all_files_concat_deduplicated.copy()\n",
    "    df_all_files_concat_deduplicated['versionID'] = df_all_files_concat_deduplicated['versionID'].replace(9999999, None)\n",
    "    df_all_authors_concat = pd.concat([df_author_entries, df_author_entries_versions], ignore_index=True)\n",
    "    df_all_authors_concat_deduplicated = df_all_authors_concat.drop_duplicates(subset=['doi', 'authorName', 'authorAffiliation', 'currentStatus'], keep='first')\n",
    "else:\n",
    "    #sort on status and then total version, setting 'DRAFT' at bottom to remove this version for published datasets that are in draft state, retain entry of 'PUBLISHED' and then to keep the earliest version\n",
    "    df_native_pruned = df_native_pruned.sort_values(by=['currentStatus', 'totalVersion'], ascending=[False, True])\n",
    "    df_all_files_concat_deduplicated = df_native_pruned.drop_duplicates(subset=['storageIdentifier'], keep='first')\n",
    "    df_all_authors_concat_deduplicated = df_author_entries.drop_duplicates(subset=['doi', 'authorName', 'authorAffiliation', 'currentStatus'], keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdaacf41",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f0294c2",
   "metadata": {},
   "source": [
    "### Metadata assessment: part 3\n",
    "\n",
    "Finally we have reached the part that is probably most interesting to some. A wide range of metadata assessments are made here (regardless of whether you queried the Versions endpoint). This includes:\n",
    "\n",
    "* Whether a file includes 'readme', 'codebook', or 'dictionary' (proxy for data dictionary) in the filename\n",
    "* Converting mimeTypes to friendlyFormats; the Dataverse API does have an existing field for this, which is included in the subsetted outputs, but I find that it still needs work to standardize, and I had this giant key-value list anyway from a different script that uses APIs that don't have friendlyFormat information.\n",
    "* Whether a file is a software format (e.g., Python, R, C++)\n",
    "* Whether a file is a compressed archive (e.g., .zip, .gzip, .tar)\n",
    "* Whether a file is a Microsoft Office format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "acc0410d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#metadata assessment\n",
    "##readme presence\n",
    "df_all_files_concat_deduplicated.loc[:,'isREADME'] = df_all_files_concat_deduplicated['filename'].str.contains('readme', case=False)\n",
    "df_all_files_concat_deduplicated.loc[:,'isCodebook'] = df_all_files_concat_deduplicated['filename'].str.contains('codebook', case=False)\n",
    "df_all_files_concat_deduplicated.loc[:,'isDataDict'] = df_all_files_concat_deduplicated['filename'].str.contains('dictionary', case=False) #need to check sensitivity\n",
    "\n",
    "##create separate friendlyFormat column\n",
    "formatMap = config['FORMAT_MAP']\n",
    "df_all_files_concat_deduplicated.loc[:,'friendlyFormat_manual'] = df_all_files_concat_deduplicated['mimeType'].apply(\n",
    "    lambda x: formatMap.get(x.strip(), x.strip()) if isinstance(x, str) and x != \"no match found\" else \"no files\"\n",
    ")\n",
    "##file formats\n",
    "softwareFormats = set(config['SOFTWARE_FORMATS'].keys())\n",
    "compressedFormats = set(config['COMPRESSED_FORMATS'].keys())\n",
    "microsoftFormats = set(config['MICROSOFT_FORMATS'].keys())\n",
    "# Assume softwareFormats is a set of friendly software format names\n",
    "df_all_files_concat_deduplicated.loc[:,'isSoftware'] = df_all_files_concat_deduplicated['mimeType'].apply(\n",
    "    lambda x: any(part.strip() in softwareFormats for part in x.split(';')) if isinstance(x, str) else False\n",
    ")\n",
    "df_all_files_concat_deduplicated.loc[:,'isCompressed'] = df_all_files_concat_deduplicated['mimeType'].apply(\n",
    "    lambda x: any(part.strip() in compressedFormats for part in x.split(';')) if isinstance(x, str) else False\n",
    ")\n",
    "df_all_files_concat_deduplicated.loc[:,'isMSOffice'] = df_all_files_concat_deduplicated['mimeType'].apply(\n",
    "    lambda x: any(part.strip() in microsoftFormats for part in x.split(';')) if isinstance(x, str) else False\n",
    ")\n",
    "\n",
    "df_all_files_concat_deduplicated.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-files-deduplicated.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77dd43",
   "metadata": {},
   "source": [
    "Now you might want to combine all of the files for a given dataset into a single entry; this would be important for things like analyzing total deposit size (inclusive of all versions) or otherwise doing some of the metadata assessments at the dataset level. The following code block does that. It first defines the column(s) to be mathematically added together (only one in this case); all others will be combined into a semi-colon-delimited string. There is also some metadata editing to clean up entries (e.g., when a recombined dataset record contains 'False; True' for a Boolean variable, this is converted to 'TRUE'). For variables where some values could be duplicated (e.g., 100 text files, only a set [unique values] is returned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "91ea9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_columns = ['fileSize']\n",
    "\n",
    "def agg_func(column_name):\n",
    "    if column_name in sum_columns:\n",
    "        return 'sum'\n",
    "    else:\n",
    "        return lambda x: sorted(set(map(str, x)))\n",
    "\n",
    "agg_funcs = {col: agg_func(col)for col in df_all_files_concat_deduplicated.columns if col != 'datasetID'}\n",
    "\n",
    "df_tdr_all_files_combined = df_all_files_concat_deduplicated.groupby('datasetID').agg(agg_funcs).reset_index()\n",
    "# Convert all list-type columns to comma-separated strings\n",
    "for col in df_tdr_all_files_combined.columns:\n",
    "    if df_tdr_all_files_combined[col].apply(lambda x: isinstance(x, list)).any():\n",
    "        df_tdr_all_files_combined[col] = df_tdr_all_files_combined[col].apply(lambda x: '; '.join(map(str, x)))\n",
    "\n",
    "tdr_all_datasets_deduplicated = df_tdr_all_files_combined.drop_duplicates(subset='datasetID', keep='first')\n",
    "tdr_all_datasets_deduplicated_pruned = tdr_all_datasets_deduplicated[[\"datasetID\", \"versionID\", \"totalVersion\", \"mimeType\", \"friendlyType\", \"fileSize\", \"creationDate\", \"publicationDate\", \"institution\", \"doi\", \"datasetTitle\", \"dataverse\", \"creationYear\", \"publicationYear\", \"isREADME\", \"isCodebook\", \"isDataDict\", \"friendlyFormat_manual\", \"isSoftware\", \"isCompressed\", \"isMSOffice\"]]\n",
    "\n",
    "#handles entries where aggregation returned a mixed 'False;True' value\n",
    "def normalize_boolean_column(col):\n",
    "    return col.apply(lambda x: True if isinstance(x, str) and 'true' in x.lower() else False)\n",
    "bool_columns = [\"isREADME\", \"isCodebook\", \"isDataDict\", \"isSoftware\", \"isCompressed\", \"isMSOffice\"]\n",
    "tdr_all_datasets_deduplicated_pruned = tdr_all_datasets_deduplicated_pruned.copy()\n",
    "for col in bool_columns:\n",
    "    tdr_all_datasets_deduplicated_pruned[col] = normalize_boolean_column(tdr_all_datasets_deduplicated_pruned[col])\n",
    "tdr_all_datasets_deduplicated_pruned = tdr_all_datasets_deduplicated_pruned.rename(columns={'isREADME': 'containsREADME', 'isCodebook': 'containsCodebook', 'isDataDict': 'containsDataDict', 'isSoftware': 'containsSoftware', 'isCompressed': 'containsCompressed', 'isMSOffice': 'containsMSOffice', 'fileSize': 'datasetSize'})\n",
    "\n",
    "#returns only the highest value for the version number\n",
    "def extract_max_version(val):\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            versions = [float(v.strip()) for v in val.split(';')]\n",
    "            return max(versions)\n",
    "        except ValueError:\n",
    "            return val  # In case of unexpected format\n",
    "    return val\n",
    "tdr_all_datasets_deduplicated_pruned['totalVersion'] = tdr_all_datasets_deduplicated_pruned['totalVersion'].apply(extract_max_version)\n",
    "\n",
    "#binning datasets by size\n",
    "tdr_all_datasets_deduplicated_pruned = assign_size_bins(tdr_all_datasets_deduplicated_pruned, column='datasetSize', new_column='datasetSizeBin')\n",
    "\n",
    "tdr_all_datasets_deduplicated_pruned.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-datasets-combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85644789",
   "metadata": {},
   "source": [
    "### Metadata summary\n",
    "\n",
    "The script also returns two summary files that are likely to be of interest. The first summary is the amount of storage created by calendar year. If these numbers are discordant with previous estimates, there is a very good chance that this is because older versions are not being accounted for (either that files have been replaced or that files existed in the system well before their latest publication). For instance, if you're not paying attention, a versioned dataset most recently published in 2023 but that had a 10 GB file published in 2021, could be mistakenly counted for 2023. The second summary is the number of unique instances of file formats. The way that this calculation is done, it counts each file type once per dataset, so a dataset with 2,000 CSV files and a dataset with 1 CSV file are each counted as '1' for the total number of datasets with CSV files; this is done to eliminate the discipline-specific variability in repetitive file formats (e.g., simulation datasets with millions of text files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02470dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual size summary\n",
      "   creationYear      fileSize      fileGB\n",
      "0          2017  1.838593e+09    1.838593\n",
      "1          2018  1.349090e+09    1.349090\n",
      "2          2019  2.931868e+08    0.293187\n",
      "3          2020  7.414379e+08    0.741438\n",
      "4          2021  1.226010e+10   12.260102\n",
      "5          2022  3.301655e+08    0.330166\n",
      "6          2023  1.336943e+10   13.369429\n",
      "7          2024  7.977032e+11  797.703152\n",
      "8          2025  5.840799e+09    5.840799\n"
     ]
    }
   ],
   "source": [
    "#size summary\n",
    "size_by_year = df_all_files_concat_deduplicated.groupby('creationYear')['fileSize'].sum().reset_index()\n",
    "size_by_year['fileGB'] = size_by_year['fileSize'] / 1000000000\n",
    "print('Annual size summary')\n",
    "print(size_by_year)\n",
    "size_by_year.to_csv(f\"outputs/{todayDate}_{institutionFilename}_annual-size-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "466ef14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total file format summary\n",
      "friendlyFormat_manual\n",
      "AVI                           1\n",
      "Binary                        9\n",
      "C Source                      1\n",
      "CSV                          12\n",
      "DVB Subtitle                  1\n",
      "Fixed Field                   2\n",
      "GZIP (compressed archive)     1\n",
      "HDF5                          3\n",
      "JPEG                          3\n",
      "Jupyter Notebook              1\n",
      "KMZ                           1\n",
      "MATLAB                        1\n",
      "MP4                           2\n",
      "MS Excel                      1\n",
      "MS PowerPoint                 1\n",
      "MS Shortcut                   1\n",
      "MS Word                       6\n",
      "Makefile                      1\n",
      "Markdown                      2\n",
      "MiniSEED                      1\n",
      "NetCDF                        4\n",
      "PDF                          16\n",
      "PNG                           6\n",
      "Pascal                        1\n",
      "Plain text                   12\n",
      "PostScript                    1\n",
      "Python                        2\n",
      "R Notebook                    3\n",
      "R Syntax                      4\n",
      "SVG                           1\n",
      "TAR (compressed archive)      1\n",
      "TIFF                          9\n",
      "TSV                          20\n",
      "YAML                          1\n",
      "ZIP (compressed archive)      4\n",
      "Zipped Shapefile              2\n",
      "Name: datasetID, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#file format summary\n",
    "##can substitute 'friendlyType' for 'mimeType' but will get some aggregating into 'unknown'\n",
    "unique_datasets_per_format = df_all_files_concat_deduplicated.groupby('friendlyFormat_manual')['datasetID'].nunique()\n",
    "print('Total file format summary')\n",
    "print(unique_datasets_per_format)\n",
    "unique_datasets_per_format.to_csv(f\"outputs/{todayDate}_{institutionFilename}_unique-format-summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d0ae6d",
   "metadata": {},
   "source": [
    "### Metadata assessment: part 4\n",
    "\n",
    "The last metadata assessment is for the author-level data. The script assesses the following:\n",
    "\n",
    "* Whether an author has a ROR-linked affiliation\n",
    "* Whether an author has a properly formatted ORCID (in Dataverse, this means that the ORCID is hyperlinked and that it appears in two fields)\n",
    "* Whether an author has an ORCID, but it is not properly formatted because it lacks hyphens\n",
    "* Whether an author has an ORCID, but it is not properly formatted because it is not in URL form\n",
    "* Whether an author has an ORCID, but it is not properly formatted because it appears in only one of the two fields\n",
    "* Whether an author has an ORCID, but it is malformed in some fashion (if any of the previous three are TRUE)\n",
    "* Whether an author's name appears malformed (First Last rather than Last, First); this is based on whether there is a space in the name value but no comma\n",
    "* Whether an author's initial appears malformed (probably middle, but could be first); this is based on whether there is a standalone letter with no period after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a0cc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#author assessment\n",
    "##is ROR present\n",
    "df_all_authors_concat_deduplicated = df_all_authors_concat_deduplicated.copy()\n",
    "df_all_authors_concat_deduplicated.loc[:, 'missingROR'] = (df_all_authors_concat_deduplicated['rorID'].isna() | (df_all_authors_concat_deduplicated['rorID'] == ''))\n",
    "##is any author ID system present\n",
    "df_all_authors_concat_deduplicated.loc[:, 'missingAuthorScheme'] = (df_all_authors_concat_deduplicated['authorIdentifierScheme'].isna() |\n",
    "    (df_all_authors_concat_deduplicated['authorIdentifierScheme'] == ''))\n",
    "##ORCID present and appropriately formatted\n",
    "df_all_authors_concat_deduplicated.loc[:, 'properORCID'] = (\n",
    "    df_all_authors_concat_deduplicated['authorIdentifierScheme'].str.upper() == 'ORCID'\n",
    ") & df_all_authors_concat_deduplicated['authorIdentifier'].str.contains('https://orcid.org/', na=False)\n",
    "##is ORCID present but malformatted (not hyperlinked)\n",
    "df_all_authors_concat_deduplicated.loc[:,'malformedORCID_noHyphens'] = (\n",
    "    df_all_authors_concat_deduplicated['authorIdentifierScheme'].str.upper() == 'ORCID'\n",
    ") & ~df_all_authors_concat_deduplicated['authorIdentifier'].str.contains('-', na=False)\n",
    "##is ORCID present but malformatted (no dashes)\n",
    "df_all_authors_concat_deduplicated.loc[:,'malformedORCID_noURL'] = (\n",
    "    df_all_authors_concat_deduplicated['authorIdentifierScheme'].str.upper() == 'ORCID'\n",
    ") & ~df_all_authors_concat_deduplicated['authorIdentifier'].str.contains('https://orcid.org/', na=False)\n",
    "##is ORCID present but malformatted (single field)\n",
    "df_all_authors_concat_deduplicated.loc[:,'malformedORCID_singleField'] = (\n",
    "    df_all_authors_concat_deduplicated['authorIdentifierScheme'].str.upper() == 'ORCID'\n",
    ") & df_all_authors_concat_deduplicated['authorIdentifierExpanded'].isna()\n",
    "\n",
    "df_all_authors_concat_deduplicated.loc[:, 'malformedORCID_any'] = (\n",
    "    df_all_authors_concat_deduplicated['malformedORCID_noHyphens'] |\n",
    "    df_all_authors_concat_deduplicated['malformedORCID_noURL'] |\n",
    "    df_all_authors_concat_deduplicated['malformedORCID_singleField']\n",
    ")\n",
    "##malformed author name (order)\n",
    "df_all_authors_concat_deduplicated.loc[:, 'malformedOrder'] = (\n",
    "    df_all_authors_concat_deduplicated['authorName'].str.contains(' ', na=False) & \n",
    "    ~df_all_authors_concat_deduplicated['authorName'].str.contains(',', na=False)\n",
    ")\n",
    "##malformed initial (standalone initial without period)\n",
    "df_all_authors_concat_deduplicated.loc[:, 'malformedInitial'] = df_all_authors_concat_deduplicated['authorName'].str.contains(r'\\b[A-Z]\\b(?!\\.)', regex=True)\n",
    "\n",
    "df_all_authors_concat_deduplicated.loc[:, 'malformedName'] = (\n",
    "    df_all_authors_concat_deduplicated['malformedOrder'] |\n",
    "    df_all_authors_concat_deduplicated['malformedInitial'] \n",
    ")\n",
    "\n",
    "df_all_authors_concat_deduplicated = df_all_authors_concat_deduplicated.sort_values(by='authorName')\n",
    "df_all_authors_concat_deduplicated.to_csv(f'outputs/{todayDate}_{institutionFilename}_all-authors.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e196e7b3",
   "metadata": {},
   "source": [
    "If you aren't a superuser, running this script across some/all TDR institutions will overweight the results in favour of your home institution because you won't pick up unpublished datasets for other institutions. The following code block uses the *excludeDrafts* toggle to determine whether to export versions of the final output files with unpublished datasets/files removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2b5761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if excludeDrafts:\n",
    "    #authors\n",
    "    df_all_authors_concat_deduplicated_published = df_all_authors_concat_deduplicated[df_all_authors_concat_deduplicated['currentStatus'] != 'DRAFT']\n",
    "    df_all_authors_concat_deduplicated_published.to_csv(f'outputs/{todayDate}_{institutionFilename}_all-authors-PUBLISHED.csv', index=False)\n",
    "\n",
    "    #datasets\n",
    "    tdr_all_datasets_deduplicated_pruned_published = tdr_all_datasets_deduplicated_pruned[tdr_all_datasets_deduplicated_pruned['publicationDate'].notna() & (tdr_all_datasets_deduplicated_pruned['publicationDate'] != '')]\n",
    "    tdr_all_datasets_deduplicated_pruned_published.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-datasets-combined-PUBLISHED.csv\")\n",
    "\n",
    "    #files\n",
    "    df_all_files_concat_deduplicated_published = df_all_files_concat_deduplicated[df_all_files_concat_deduplicated['publicationDate'].notna() & (df_all_files_concat_deduplicated['publicationDate'] != '')]\n",
    "    df_all_files_concat_deduplicated_published.to_csv(f\"outputs/{todayDate}_{institutionFilename}_all-files-deduplicated-PUBLISHED.csv\")\n",
    "\n",
    "    #size summary\n",
    "    size_by_year_published = df_all_files_concat_deduplicated_published.groupby('creationYear')['fileSize'].sum().reset_index()\n",
    "    size_by_year_published['fileGB'] = size_by_year_published['fileSize'] / 1000000000\n",
    "    size_by_year_published.to_csv(f\"outputs/{todayDate}_{institutionFilename}_annual-size-summary-PUBLISHED.csv\")\n",
    "\n",
    "    #file format summary\n",
    "    ##can substitute 'friendlyType' for 'mimeType' but will get some aggregating into 'unknown'\n",
    "    unique_datasets_per_format_PUBLISHED = df_all_files_concat_deduplicated_published.groupby('friendlyFormat_manual')['datasetID'].nunique()\n",
    "    unique_datasets_per_format_PUBLISHED.to_csv(f\"outputs/{todayDate}_{institutionFilename}_unique-format-summary-PUBLISHED.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5f0ac12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "\n",
      "Time to run: 0:00:56.841205\n",
      "\n",
      "**REMINDER: THIS IS A TEST RUN, AND ANY RESULTS ARE NOT COMPLETE!**\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Done.\\n\")\n",
    "print(f\"Time to run: {datetime.now() - startTime}\\n\")\n",
    "if test:\n",
    "    print(\"**REMINDER: THIS IS A TEST RUN, AND ANY RESULTS ARE NOT COMPLETE!**\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
